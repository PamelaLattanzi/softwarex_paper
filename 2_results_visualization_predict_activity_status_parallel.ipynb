{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d17dcb",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Predicting activity status: models & variables combinations to test](#predicting-activity-status-models--variables-combinations-to-test)\n",
    "  - [Import libraries](#import-libraries)\n",
    "  - [Set the directory](#set-the-directory)\n",
    "  - [Import the final dataset](#import-the-final-dataset)\n",
    "  - [Train vs. validation vs. test sets](#train-vs-validation-vs-test-sets)\n",
    "  - [PCA biplot - representation of relationships observed between metrics (sample by boat trip) and models](#pca-biplot---representation-of-relationships-observed-between-metrics-sample-by-boat-trip-and-models)\n",
    "  - [Calculating Global SHAP Feature Importance](#calculating-global-shap-feature-importance)\n",
    "  - [Calculating Aggregated SHAP Feature Importance](#calculating-aggregated-shap-feature-importance)\n",
    "  - [Generating the ROC curves](#generating-the-roc-curves)\n",
    "  - [Mapping models' predictions of *fishing/not_fishing* for each trip](#mapping-models-predictions-of-fishingnotfishing-for-each-trip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95816a",
   "metadata": {},
   "source": [
    "# Predicting activity status: models & variables combinations to test\n",
    "Analysis of \"1_predict_activity_status_parallel.py\" output <br>\n",
    "\n",
    "The following models were tested as classifiers, aiming at predicting the activity status: recognising if the fisher is fishing - hauling the gear - or not - setting the gear/navigation. <br>\n",
    "1. **LoRe**: Logistic Regression;\n",
    "2. **Dtree**: Decision Trees;\n",
    "3. **RaFo**: Random Forests;\n",
    "4. **XGBo**: Extreme Gradient Boosting. <br>\n",
    "\n",
    "Different metrics were calculated to provide various perspectives on the performance of the tested models. <br>\n",
    "1. **Accuracy**: the proportion of correctly classified instances (both positive and negative) out of the total number of instances.\n",
    "2. **Precision**: the proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
    "3. **Recall** (also called sensitivity): the proportion of correctly predicted positive instances out of all actual positive instances.\n",
    "4. **F1 score**: the harmonic mean of precision and recall.\n",
    "5. **AUC score**:  the area under the Receiver Operating Characteristic (ROC) curve.\n",
    "6. **Specificity**: the proportion of correctly predicted negative instances out of all actual negative instances. <br>\n",
    "\n",
    "Furthermore, the same models and the same metrics have been tested and calculated with 7 variables, used as predictors to guide the models recognising the activity status.\n",
    "* 7-variables: SPEED, course_diff, distance_from_coast (OR depth), hours, time_seconds, months, trip_duration  <br>\n",
    "'hour' and 'months' were categorigal variables, one-hot encoded to make the models perceive them as factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc43ce-830a-4b5a-a78b-ccbc2c9ab21e",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce639d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import ast\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pyreadr\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d080048",
   "metadata": {},
   "source": [
    "## Set the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f621124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "basedir = \"path/to/your_folder/\"  # must contain this script and the dataset\n",
    "outdir = \"results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3d3c9",
   "metadata": {},
   "source": [
    "## Import the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the output dataset from the Python script \"1_predict_activity_status_parallel.py\"\n",
    "table = pd.read_csv(os.path.join(basedir,outdir, \"model_performances_on_data_splitting_with_shap.csv\"))\n",
    "table['num_var'] = table['combo'].str.split(',').str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf8b6f0",
   "metadata": {},
   "source": [
    "## Train vs. validation vs. test sets\n",
    "\n",
    "Let's plot the results recorded for the different calculated metrics, comparing performances among the three sets: train, validation and test. <br>\n",
    "\n",
    "* **Train set** = **70%** of the fishing trips\n",
    "* **Validation set** = **20%** of the fishing trips\n",
    "* **Test set** = **10%** of the fishing trips (unseen data) <br>\n",
    "\n",
    "Sample unit by boat should be used, as the sample unit by point can be too optimistic. Indeed, the latter uses random points from different trips to train the models instead of using the full trips, which is unrealistic for the type of data that we are working with (time-series), as stated by Samarão *et al.*, 2024. <br>\n",
    "\n",
    "The following plot shows performances of the 6 listed evaluation metrics for the 4 aforementioned classification models, with data splitting into train, validation and test sets. Boxplots display the distribution of training (blue) and validation (green) scores across cross-validation folds. The blue and green numeric labels indicate the mean score for their respective sets. The red diamond and its red numeric label show the final, single score achieved on the independent test set. A 90% threshold (red dashed line) is included for reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = pd.DataFrame()\n",
    "metrics = ['accuracy', 'precision', 'recall','f1', 'specificity', 'auc']\n",
    "\n",
    "# Reshape and compute summary statistics\n",
    "for metric in metrics:\n",
    "    temp = table[['model', 'num_var', f'{metric}_train', f'{metric}_val', f'{metric}_test']].copy()\n",
    "    temp = temp.melt(id_vars=['model', 'num_var'],\n",
    "                     value_vars=[f'{metric}_train', f'{metric}_val', f'{metric}_test'],\n",
    "                     var_name='set',\n",
    "                     value_name='value')\n",
    "    temp['metric'] = metric\n",
    "    temp['set'] = temp['set'].str.extract(r'_(train|val|test)')\n",
    "    summary_data = pd.concat([summary_data, temp], ignore_index=True)\n",
    "\n",
    "# Filter out empty cells\n",
    "summary_data = summary_data[summary_data['value'].notna()]\n",
    "\n",
    "def get_whisker_max(data):\n",
    "    \"\"\"Calculates the maximum value displayed by the upper whisker (Q3 + 1.5*IQR).\"\"\"\n",
    "    if data.empty:\n",
    "        return np.nan\n",
    "    q3 = data.quantile(0.75)\n",
    "    iqr = q3 - data.quantile(0.25)\n",
    "    # Max value is the maximum data point that is <= (Q3 + 1.5 * IQR)\n",
    "    whisker_max = data[data <= (q3 + 1.5 * iqr)].max()\n",
    "    # Handle cases where all data might be outliers, return Q3 in a pinch\n",
    "    return whisker_max if not pd.isna(whisker_max) else q3\n",
    "\n",
    "set_order = ['train', 'val', 'test']\n",
    "summary_data['set'] = pd.Categorical(summary_data['set'], categories=set_order, ordered=True)\n",
    "\n",
    "model_order = ['LoRe', 'Dtree', 'RaFo', 'XGBo']\n",
    "metric_order = ['accuracy', 'precision', 'recall', 'f1','specificity', 'auc']\n",
    "summary_data['model'] = pd.Categorical(summary_data['model'], categories=model_order, ordered=True)\n",
    "summary_data['metric'] = pd.Categorical(summary_data['metric'], categories=metric_order, ordered=True)\n",
    "\n",
    "metric_title_map = {m: m.capitalize() for m in metric_order}\n",
    "summary_data['metric_capitalized'] = summary_data['metric'].map(metric_title_map)\n",
    "summary_data['metric_capitalized'] = pd.Categorical(\n",
    "    summary_data['metric_capitalized'],\n",
    "    categories=[metric_title_map[m] for m in metric_order],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "boxplot_data = summary_data[summary_data['set'].isin(['train', 'val'])].copy()\n",
    "point_data = summary_data[summary_data['set'] == 'test'].copy()\n",
    "mean_data = boxplot_data.groupby(['model', 'metric', 'set'])['value'].mean().reset_index()\n",
    "\n",
    "\n",
    "# PERSONALIZED OFFSETS --------------------------------\n",
    "# We use a single offset for train/val/test, applied to the calculated Y position\n",
    "label_offset = 0.005 \n",
    "label_format = \"{:.2f}\" \n",
    "# Offsets for X-axis positioning\n",
    "x_offset_standard = 0.25\n",
    "x_pos_train = -x_offset_standard      \n",
    "x_pos_test = x_offset_standard        \n",
    "x_pos_val = 0.00 \n",
    "\n",
    "\n",
    "# PLOT GENERATION: USING PLT.SUBPLOTS ----------------\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "n_metrics = len(metric_order)\n",
    "n_cols = 6\n",
    "n_rows = 1 \n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, 7), sharey=False) \n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "box_handles = []\n",
    "box_labels = []\n",
    "\n",
    "for i, metric in enumerate(metric_order):\n",
    "    ax = axes[i]\n",
    "    metric_title = metric_title_map[metric]\n",
    "    \n",
    "    metric_data_box = boxplot_data[boxplot_data['metric'] == metric]\n",
    "    metric_data_mean = mean_data[mean_data['metric'] == metric]\n",
    "    metric_data_test = point_data[point_data['metric'] == metric]\n",
    "\n",
    "    # Plot the Boxplot \n",
    "    legend_status = True if i == 0 else False\n",
    "    sns.boxplot(data=metric_data_box, x='model', y='value', hue='set',\n",
    "                palette='viridis', showfliers=False, width=0.7, ax=ax,\n",
    "                boxprops=dict(alpha=0.8),\n",
    "                medianprops=dict(color='black', linewidth=2),\n",
    "                legend=legend_status)\n",
    "\n",
    "    # Add Numeric Labels (Test and Mean for Train/Val)\n",
    "    for j, model in enumerate(model_order):\n",
    "        \n",
    "        # --- TEST SET VALUE ---\n",
    "        model_point = metric_data_test[metric_data_test['model'] == model]\n",
    "        if len(model_point) == 1:\n",
    "            test_value = model_point['value'].iloc[0]\n",
    "\n",
    "            ax.scatter(j + x_pos_test, test_value, color='red', marker='D', s=70,\n",
    "                        edgecolors='black', linewidths=1,\n",
    "                        label='test' if (i == 0 and j == 0) else None) \n",
    "            \n",
    "            # Position is test_value + offset\n",
    "            ax.text(j + x_pos_test, test_value + label_offset,\n",
    "                        label_format.format(test_value),\n",
    "                        color='red', ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "\n",
    "        # --- TRAIN SET MEAN LABEL (Overlap prevention) ---\n",
    "        mean_train_df = metric_data_mean[(metric_data_mean['model'] == model) & (mean_data['set'] == 'train')]\n",
    "        train_full_data = summary_data[(summary_data['model'] == model) & (summary_data['metric'] == metric) & (summary_data['set'] == 'train')]['value']\n",
    "        \n",
    "        if not mean_train_df.empty:\n",
    "            mean_train = mean_train_df['value'].iloc[0]\n",
    "            whisker_max = get_whisker_max(train_full_data)\n",
    "            \n",
    "            # The Y position is the maximum of the mean or the whisker max, plus offset\n",
    "            y_pos_train = max(mean_train, whisker_max) + label_offset\n",
    "\n",
    "            ax.text(j + x_pos_train, y_pos_train,\n",
    "                        label_format.format(mean_train), # We still plot the mean value\n",
    "                        color='blue', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "        # --- VALIDATION SET MEAN LABEL (Overlap prevention) ---\n",
    "        mean_val_df = metric_data_mean[(metric_data_mean['model'] == model) & (mean_data['set'] == 'val')]\n",
    "        val_full_data = summary_data[(summary_data['model'] == model) & (summary_data['metric'] == metric) & (summary_data['set'] == 'val')]['value']\n",
    "        \n",
    "        if not mean_val_df.empty:\n",
    "            mean_val = mean_val_df['value'].iloc[0]\n",
    "            whisker_max = get_whisker_max(val_full_data)\n",
    "            \n",
    "            # The Y position is the maximum of the mean or the whisker max, plus offset\n",
    "            y_pos_val = max(mean_val, whisker_max) + label_offset\n",
    "\n",
    "            ax.text(j + x_pos_val, y_pos_val,\n",
    "                        label_format.format(mean_val),\n",
    "                        color='green', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "\n",
    "    # Final Axis Styling\n",
    "    ax.set_ylim(0.8, 1)\n",
    "    ax.set_yticks(np.arange(0.75, 1.01, 0.05))\n",
    "    ax.set_xlabel(\"\") \n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.axhline(0.9, color='red', linestyle='--', linewidth=1) \n",
    "    ax.set_ylabel(metric_title, fontsize=14, labelpad=10, weight='bold') \n",
    "    \n",
    "    if i > 0:\n",
    "        ax.tick_params(axis='y', labelleft=False)\n",
    "    if i == 0:\n",
    "        box_handles, box_labels = ax.get_legend_handles_labels()\n",
    "        if ax.legend_ is not None:\n",
    "            ax.legend_.remove()\n",
    "    elif ax.legend_ is not None:\n",
    "        ax.legend_.remove() \n",
    "\n",
    "\n",
    "# Global Legend --------------------------------------\n",
    "cleaned_box_handles = [h for h, l in zip(box_handles, box_labels) if l in ['train', 'val']]\n",
    "cleaned_box_labels = [l for l in box_labels if l in ['train', 'val']]\n",
    "\n",
    "test_handle_fixed = plt.Line2D([], [], \n",
    "                               color='red', \n",
    "                               marker='D',          \n",
    "                               linestyle='None', \n",
    "                               markersize=8, \n",
    "                               label='test', \n",
    "                               markerfacecolor='red', \n",
    "                               markeredgecolor='black', \n",
    "                               markeredgewidth=1)      \n",
    "\n",
    "final_handles = cleaned_box_handles + [test_handle_fixed]\n",
    "final_labels = cleaned_box_labels + ['test']\n",
    "\n",
    "fig.legend(title=\"Dataset Split\", handles=final_handles, labels=final_labels,\n",
    "           bbox_to_anchor=(1.02, 0.5), loc='center left', borderaxespad=0.)\n",
    "\n",
    "# Final Layout and Save\n",
    "plt.tight_layout(rect=[0, 0, 0.95, 1])\n",
    "filename = \"hauls_multi_class_performance_boxplots_with_test.png\"\n",
    "full_path = os.path.join(basedir, outdir, filename)\n",
    "plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21717602",
   "metadata": {},
   "source": [
    "## Principal Components Analysis (PCA) on the test set\n",
    "\n",
    "This chunk performs a Principal Component Analysis (PCA) on standardized performance metrics (accuracy, precision, recall, F1-score, specificity, and AUC) computed on the final test set. After removing incomplete cases, the metrics are scaled and used to derive principal components. \n",
    "* A **PCA biplot** is produced to visualize and compare model performance in the space of the first two components, showing both model scores and variable loadings. \n",
    "* In addition, a **scree plot** and **cumulative explained variance plot** are generated to assess the contribution of each component and determine how much variance is captured by the leading components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = table.copy()\n",
    "test_results_df = results_df[results_df['fold'] == 'final_test'].copy()\n",
    "metrics_for_pca = ['accuracy_test', 'precision_test', 'recall_test','f1_test', 'specificity_test', 'auc_test']\n",
    "\n",
    "test_results_pca = test_results_df.dropna(subset=metrics_for_pca).copy()\n",
    "\n",
    "if test_results_pca.empty:\n",
    "    print(\"No valid test set metrics found for PCA.\")\n",
    "else:\n",
    "    # PCA Calculation\n",
    "    X = test_results_pca[metrics_for_pca]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    pca = PCA()\n",
    "    principal_components = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    pca_df = pd.DataFrame(\n",
    "        data=principal_components[:, :2], \n",
    "        columns=['PC1', 'PC2']\n",
    "    )\n",
    "    pca_df = pd.concat([test_results_pca[['model']].reset_index(drop=True), pca_df], axis=1)\n",
    "\n",
    "    ### 1. PCA biplot ###\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    sns.scatterplot(x='PC1', y='PC2', hue='model', data=pca_df, s=200, marker='o', zorder=3)\n",
    "\n",
    "    # Add Scaled Arrows - adjusts arrow length to match the scale of the PC scores \n",
    "    loadings = pca.components_\n",
    "    scale_factor = np.max(np.abs(principal_components[:, :2])) / np.max(np.abs(loadings[:2, :])) * 0.8\n",
    "\n",
    "    for i, feature in enumerate(metrics_for_pca):\n",
    "        x_arr = loadings[0, i] * scale_factor\n",
    "        y_arr = loadings[1, i] * scale_factor\n",
    "        plt.arrow(0, 0, x_arr, y_arr, color='r', alpha=0.5, linewidth=1.5, head_width=0.1, head_length=0.15)\n",
    "        plt.text(x_arr * 1.1, y_arr * 1.1, feature, color='r', alpha=0.8, fontsize=11, fontweight='bold')\n",
    "\n",
    "    plt.axhline(y=0, color='grey', linewidth=1, linestyle='--')\n",
    "    plt.axvline(x=0, color='grey', linewidth=1, linestyle='--')\n",
    "    plt.title(f'PCA Biplot: Model Performance (Test Set)\\nPC1: {pca.explained_variance_ratio_[0]:.1%} | PC2: {pca.explained_variance_ratio_[1]:.1%}')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    filename = \"PCA_biplot.png\"\n",
    "    full_path = os.path.join(basedir, outdir, filename)\n",
    "    plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    ### 2. Scree Plot & Cumulative Variance ###\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Scree Plot (Eigenvalues)\n",
    "    eigenvalues = pca.explained_variance_\n",
    "    ax1.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', color='steelblue', linewidth=2)\n",
    "    ax1.axhline(y=1, color='r', linestyle='--', label='Kaiser Criterion (λ=1)')\n",
    "    ax1.set_title('Scree Plot (Importance of Components)')\n",
    "    ax1.set_xlabel('Principal Component')\n",
    "    ax1.set_ylabel('Eigenvalue')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Cumulative Explained Variance\n",
    "    cum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    ax2.plot(range(1, len(cum_variance) + 1), cum_variance, marker='o', color='seagreen', linewidth=2)\n",
    "    ax2.axhline(y=0.95, color='orange', linestyle=':', label='95% Threshold')\n",
    "    ax2.set_title('Cumulative Explained Variance')\n",
    "    ax2.set_xlabel('Number of Components')\n",
    "    ax2.set_ylabel('Total Variance Explained (%)')\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = \"Scree_plot_and_cumulative_variance.png\"\n",
    "    full_path = os.path.join(basedir, outdir, filename)\n",
    "    plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Variance explained by PC1 + PC2: {cum_variance[1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a73fff",
   "metadata": {},
   "source": [
    "## Calculating Global SHAP Feature Importance\n",
    "This code generates global SHAP feature importance visualizations for each model evaluated on the final test set. Rows corresponding to the final test fold with available SHAP information are selected, and the stored feature importance values are parsed from their dictionary representation. For each model, features are ranked by their mean absolute SHAP values and displayed in a bar chart, highlighting the relative contribution of each feature to the model’s predictions. <br>\n",
    "Here are shown all the levels of *hours* and *months* categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.copy()\n",
    "\n",
    "# Filter for final test rows where SHAP data exists\n",
    "final_test_df = df[df['fold'] == 'final_test'].dropna(subset=['feature_importance']).copy()\n",
    "\n",
    "for idx, row in final_test_df.iterrows():\n",
    "    model_name = row['model']\n",
    "    \n",
    "    try:\n",
    "        # The 'feature_importance' column is a string representation of a dict\n",
    "        # Use ast.literal_eval to safely convert it to a real Python dictionary\n",
    "        imp_dict = ast.literal_eval(row['feature_importance'])\n",
    "        \n",
    "        # Sort features by importance value\n",
    "        sorted_imp = sorted(imp_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        features, values = zip(*sorted_imp)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(x=list(values), y=list(features), palette=\"viridis\")\n",
    "        plt.title(f'SHAP Global Feature Importance: {model_name}', fontsize=14)\n",
    "        plt.xlabel('Mean |SHAP value| (Average Impact on Model Output)')\n",
    "        plt.ylabel('Features')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        save_path = os.path.join(basedir, outdir, f'shap_importance_{model_name}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"Created shap_importance_{model_name}.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse importance for {model_name}: {e}\")\n",
    "\n",
    "print(\"\\nDone! All plots are in the 'plots' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d510fd2",
   "metadata": {},
   "source": [
    "## Calculating Aggregated SHAP Feature Importance \n",
    "This code computes and visualizes aggregated global SHAP feature importance for models evaluated on the final test set. SHAP values are parsed from their stored dictionary format and categorical features (*hours* and *months*), are collapsed by summing their respective importance values across levels. All remaining features are retained individually. The aggregated and original features are then ranked by mean absolute SHAP value and displayed in bar plots, providing a more interpretable summary of the relative influence of temporal and continuous predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_df = df[df['fold'] == 'final_test'].dropna(subset=['feature_importance']).copy()\n",
    "\n",
    "for idx, row in final_test_df.iterrows():\n",
    "    model_name = row['model']\n",
    "    \n",
    "    try:\n",
    "        raw_imp_dict = ast.literal_eval(row['feature_importance'])\n",
    "        \n",
    "        clean_imp_dict = {}\n",
    "        hours_total = 0.0\n",
    "        month_total = 0.0\n",
    "        \n",
    "        # Aggregate levels for categorical variables (hours and months)\n",
    "        for k, v in raw_imp_dict.items():\n",
    "            if k.startswith('hours_'):\n",
    "                hours_total += v\n",
    "            elif k.startswith('months_'):\n",
    "                month_total += v\n",
    "            else:\n",
    "                # Keep other variables (SPEED, distance_to_coast, etc.) as they are\n",
    "                clean_imp_dict[k] = v\n",
    "        \n",
    "        # Add the aggregated totals with your requested naming convention\n",
    "        clean_imp_dict['hours (aggregated)'] = hours_total\n",
    "        clean_imp_dict['months (aggregated)'] = month_total\n",
    "\n",
    "        # Sort features by importance value\n",
    "        sorted_imp = sorted(clean_imp_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        features, values = zip(*sorted_imp)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(x=list(values), y=list(features), palette=\"viridis\")\n",
    "        \n",
    "        plt.title(f'Aggregated SHAP Feature Importance: {model_name}', fontsize=14)\n",
    "        plt.xlabel('Mean |SHAP value| (Average Impact on Model Output)')\n",
    "        plt.ylabel('Features')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(basedir,outdir, f'shap_importance_{model_name}_aggregated.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"Created aggregated plot for {model_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a14f4f",
   "metadata": {},
   "source": [
    "## Generating the ROC curves\n",
    "\n",
    "The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate (FPR) at various threshold settings. The AUC provides a single scalar value that summarizes the overall performance of a binary classifier across all possible classification thresholds. <br>\n",
    "AUC = 1: The model perfectly distinguishes between all positive and negative instances. <br>\n",
    "AUC = 0.5: The model's performance is no better than random guessing. <br>\n",
    "AUC < 0.5: The model is performing worse than random guessing (this usually indicates an issue with the model). <br>\n",
    "Generally, the higher the AUC, the better the model's ability to discriminate between positive and negative classes. <br>\n",
    "\n",
    "ROC curves are essential because they measure a model's ability to distinguish between classes across all possible decision thresholds, rather than relying on a single fixed point. This makes them highly effective for tracking data where fishing events are often less frequent than transit. By mapping the trade-off between sensitivity and false alarms, they provide an objective Area Under the Curve (AUC) score that validates a model’s reliability. High AUC values demonstrate that the model has successfully identified complex behavioral patterns rather than biased trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the output file from the Python script \"1_predict_activity_status_parallel.py\"\n",
    "predictions = pyreadr.read_r(os.path.join(basedir, outdir, 'dataset_with_predictions.rds'))\n",
    "df = predictions[None] \n",
    "\n",
    "test_df = df[df['set'] == 'test'].copy() # we need just the predictions on unseen data\n",
    "\n",
    "# Define Confusion Categories mapping\n",
    "def get_confusion_cat(row, model_name):\n",
    "    target = row['target']\n",
    "    pred = row[f'target_pred_{model_name}']\n",
    "    if target == 0 and pred == 0: return 'TP'\n",
    "    if target == 1 and pred == 0: return 'FP'\n",
    "    if target == 1 and pred == 1: return 'TN'\n",
    "    if target == 0 and pred == 1: return 'FN'\n",
    "    return 'None'\n",
    "\n",
    "models =  ['LoRe', 'Dtree', 'RaFo', 'XGBo']\n",
    "for model in models:\n",
    "    test_df[f'cat_{model}'] = test_df.apply(lambda row: get_confusion_cat(row, model), axis=1)\n",
    "\n",
    "# Save the categorized test set\n",
    "save_path_csv = os.path.join(basedir, outdir, 'test_set_with_confusion_categories.csv')\n",
    "test_df.to_csv(save_path_csv, index=False)\n",
    "\n",
    "# Plot ROC Curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for model in models:\n",
    "    # Convert predictions and probabilities to numeric to avoid \"int and str\" errors\n",
    "    test_df[f'target_pred_{model}'] = pd.to_numeric(test_df[f'target_pred_{model}'], errors='coerce')\n",
    "    test_df[f'target_prob_{model}'] = pd.to_numeric(test_df[f'target_prob_{model}'], errors='coerce')\n",
    "    # We use 1 - prob because prob_MODEL is P(target=1), but Fishing is 0.\n",
    "    y_true = (test_df['target'] == 0).astype(int) \n",
    "    y_score = 1 - test_df[f'target_prob_{model}'] \n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{model} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate (Fishing)')\n",
    "plt.ylabel('True Positive Rate (Fishing)')\n",
    "plt.title('ROC Curves - Fishing Activity Prediction')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "save_path_roc = os.path.join(basedir, outdir, \"roc_curves.png\")\n",
    "plt.savefig(save_path_roc)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec00d64",
   "metadata": {},
   "source": [
    "## Mapping models' predictions of *fishing/not_fishing* for each trip\n",
    "\n",
    "The present chunk is useful for generating spatial maps for each test trip, visualizing model predictions along vessel trajectories. For every unique trip, geographic points are plotted in longitude–latitude space and colored according to classification outcomes (TP,FP,TN,FN). Separate panels are created for each model to enable direct comparison of spatial prediction patterns within the same trip. All trip-level maps are saved to disk, providing a visual assessment of where and how models succeed or fail in space. <br>\n",
    "\n",
    "Remember that:\n",
    "* TP = True Positives (correctly predicted positive)\n",
    "* TN = True Negatives (correctly predicted negative)\n",
    "* FP = False Positives (incorrectly predicted positive)\n",
    "* FN = False Negatives (incorrectly predicted negative) <br>\n",
    "\n",
    "Note that, in our case, TP are the fishing points (target == 0), while non-fishing points are TN (target == 1). <br>\n",
    "In the generated maps, TP/FP/TN/FN are highlighted in different colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f589ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_maps_path = os.path.join(basedir, outdir, 'trip_maps')\n",
    "if not os.path.exists(trip_maps_path):\n",
    "    os.makedirs(trip_maps_path)\n",
    "\n",
    "def save_all_trip_maps(df, models, output_dir):\n",
    "    test_trip_ids = df['boat_trip_id'].unique()\n",
    "    colors = {'TP': '#2ca02c', 'TN': '#1f77b4', 'FP': '#d62728', 'FN': '#ff7f0e'}\n",
    "    \n",
    "    print(f\"Generating {len(test_trip_ids)} trip maps...\")\n",
    "    \n",
    "    for trip_id in test_trip_ids:\n",
    "        trip_data = df[df['boat_trip_id'] == trip_id].sort_values('DATE_TIME')\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(models), figsize=(20, 8), sharex=True, sharey=True)\n",
    "        if len(models) == 1: axes = [axes]\n",
    "        \n",
    "        for ax, model in zip(axes, models):\n",
    "            for cat, color in colors.items():\n",
    "                cat_data = trip_data[trip_data[f'cat_{model}'] == cat]\n",
    "                if not cat_data.empty:\n",
    "                    ax.scatter(cat_data['longitude'], cat_data['latitude'], \n",
    "                               c=color, label=cat, s=15, alpha=0.6)\n",
    "            \n",
    "            ax.set_title(f'Trip {trip_id} Performance: {model}')\n",
    "            ax.set_xlabel('Longitude')\n",
    "            ax.set_ylabel('Latitude')\n",
    "            ax.legend(loc='best', markerscale=2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path_map = os.path.join(output_dir, f'map_trip_{trip_id}.png')\n",
    "        plt.savefig(save_path_map)\n",
    "        plt.close()\n",
    "\n",
    "save_all_trip_maps(test_df, models, trip_maps_path)\n",
    "\n",
    "print(f\"✅ Analysis complete. ROC saved to {save_path_roc} and {len(test_df['boat_trip_id'].unique())} maps saved to {trip_maps_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
